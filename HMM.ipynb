{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Two State Hidden Markov Model by Timothy Gunn <br> July 26th, 2025 <br>\n",
        "The goal of this program is to implement a two state Hidden Markov Model from scratch to practice probabilistic modeling and unsupervised learning. Given a sequence of observed emissions, the model uses the forward-backward algorithm and the Baum-Welch algorithm to recover the underlying state transition and emission probabilities without any prior knowledge of these probabilities. Although there are only 2 emission symbols and 2 hidden states in the dataset, the same algorithms still apply to data with more emissions and hidden states."
      ],
      "metadata": {
        "id": "plMOcFLv5TnU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNOyKa-Q3y_3"
      },
      "outputs": [],
      "source": [
        "#Import necessary libraries\n",
        "!pip install hmmlearn\n",
        "import numpy as np\n",
        "from hmmlearn import hmm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the true transition matrix between hidden states\n",
        "#State 0 stays in 0 with 90% probability, switches to 1 with 10%\n",
        "#State 1 stays in 1 with 90% probability, switches to 0 with 10%\n",
        "transmat_true = np.array([\n",
        "  [0.9, 0.1],\n",
        "  [0.1, 0.9]\n",
        "])\n",
        "\n",
        "#Define the true emission probabilities\n",
        "#State 0 emits observation 0 with 80% probability and 1 with 20%\n",
        "#State 1 emits observation 1 with 70% probability and 0 with 30%\n",
        "emission_probs_true = np.array([\n",
        "  [0.8, 0.2],  # State 0: 80% chance of 0\n",
        "  [0.3, 0.7]   # State 1: 70% chance of 1\n",
        "])\n",
        "\n",
        "#Define the initial state probabilities (uniform in this case)\n",
        "startprob_true = np.array([0.5, 0.5])\n",
        "\n",
        "\n",
        "#Create a Categorical Hidden Markov Model with 2 hidden states\n",
        "model_true = hmm.CategoricalHMM(\n",
        "  n_components=2, #Number of hidden states\n",
        ")\n",
        "\n",
        "#Manually set the parameters\n",
        "model_true.startprob_ = startprob_true\n",
        "model_true.transmat_ = transmat_true\n",
        "model_true.emissionprob_ = emission_probs_true\n",
        "\n",
        "#Sample 500 observations and corresponding hidden states from the model\n",
        "X, Z = model_true.sample(n_samples=500)\n",
        "\n",
        "#Output shape and a preview of the generated data\n",
        "print(\"Generated Data Shape:\", X.shape)\n",
        "print(\"First 10 Observations (X):\", X.flatten()[:10])\n",
        "print(\"First 10 Hidden States (Z):\", Z[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhFRgV6T349L",
        "outputId": "a532a298-b86f-4533-d28a-73a452d744eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Data Shape: (500, 1)\n",
            "First 10 Observations (X): [1 0 1 1 0 0 1 0 0 0]\n",
            "First 10 Hidden States (Z): [1 1 0 0 0 0 1 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Setrandom seed\n",
        "np.random.seed(42)\n",
        "\n",
        "#Compute forward probabilities (The full probability of the emitted sequence from t=0 to t=t and the probability of being in state Zt)\n",
        "#Forward Probability = Pr(X1...Xt, Zt)\n",
        "def forward(observations, a, b, pi):\n",
        "  t = len(observations) #Number of time steps\n",
        "  n = a.shape[0] #Number of hidden states\n",
        "  alpha = np.zeros((t,n)) #Initial alpha matrix (stores the forward probabilities)\n",
        "\n",
        "  #Initialization step for t=0\n",
        "  #alpha[0,i] = P(obs[0], state_0=i) = P(state_0=i) * P(obs[0] | state_0=i)\n",
        "  alpha[0] = pi * b[:, observations[0]]\n",
        "\n",
        "  #Induction step\n",
        "  #For each time step\n",
        "  for i in range(1,t):\n",
        "    #For each possible current state j\n",
        "    for j in range(n):\n",
        "      #alpha[t,j] = P(obs[0:t+1], state_t=j)\n",
        "      alpha[i,j] = np.sum(alpha[i-1] * a[:, j]) * b[j, observations[i]]\n",
        "\n",
        "  return alpha\n",
        "\n",
        "#Compute backward probabilities (The probability of seeing the rest of the sequence Xn given Z_n-1)\n",
        "def backward(observations, a, b):\n",
        "  t = len(observations) #Number of time steps\n",
        "  n = a.shape[0] #Number of hidden states\n",
        "  beta = np.zeros((t,n)) #Initialize beta matrix (stores backward probabilities)\n",
        "\n",
        "  #Initialization step for t=t-1 (the last time step)\n",
        "  #beta[T-1,i] = 1 for all states i (no future observations to consider)\n",
        "  beta[-1] = np.ones(n)\n",
        "\n",
        "  #Induction step\n",
        "  #For each time step (from t-2 to 0)\n",
        "  for i in range(t - 2, -1, -1):\n",
        "    #For each possible current state j\n",
        "    for j in range(n):\n",
        "      #beta[t,i] = P(obs[t+1:T] | state_t=i)\n",
        "      beta[i,j] = np.sum(a[j] * b[:, observations[i+1]] * beta[i+1])\n",
        "\n",
        "  return beta\n",
        "\n",
        "#Compute posterior probabilities (gamma and xi)\n",
        "def FindGammaXi(observations,a, b, pi):\n",
        "  #Number of time steps and hidden states\n",
        "  t = len(observations)\n",
        "  n = a.shape[0]\n",
        "\n",
        "  #Compute forward and backward probabilities\n",
        "  alpha = forward(observations, a, b, pi)\n",
        "  beta = backward(observations, a, b)\n",
        "\n",
        "  #Compute gamma (State posterior probabilities)\n",
        "  #gamma[t, j] = P(Q_t = j | observations, model)\n",
        "  gamma = (alpha * beta) / np.sum(alpha * beta, axis=1, keepdims=True)\n",
        "\n",
        "  #Initialize xi (Joint state-transition probabilities)\n",
        "  #xi[t, j, k] = P(Q_t = j, Q_t+1 = k | observations, model)\n",
        "  #This matrix stores the probabilities of being in state j at time t\n",
        "  #and the probabilities of transitioning to state k at time t+1, given the entire observation sequence\n",
        "  xi = np.zeros((t-1, n, n))\n",
        "\n",
        "  #For each time step\n",
        "  for i in range(t-1):\n",
        "    #Calculate the denominator d for the xi formula\n",
        "    #The denominiator represents the probability of the entire observation sequence P(observations|model)\n",
        "    d = np.sum(alpha[i][:, None] * a * b[:, observations[i+1]] * beta[i+1])\n",
        "    #Calculate xi for each possible state transition (j to k) at time i\n",
        "    for j in range(n): #Current state at time i\n",
        "      for k in range(n): #Next state at time i+1\n",
        "        xi[i,j,k] = (alpha[i, j] * a[j,k] * b[k, observations[i+1]] * beta[i+1,k]) / d\n",
        "\n",
        "  return gamma, xi\n",
        "\n",
        "#Use the gamma and chi values to update a,b, and pi\n",
        "def updateParameters(observations, gamma, xi):\n",
        "  n = gamma.shape[1] #Number of hidden states\n",
        "  m = np.max(observations) + 1 #Number possible observation symbols\n",
        "  t = len(observations) #Number of time steps\n",
        "\n",
        "  #Re-Estimate initial state probabilities (pi)\n",
        "  #The new initial probability for a given state is just the probability of being\n",
        "  #in that state at the first time step (t=0) given the observations\n",
        "  pi = gamma[0]\n",
        "\n",
        "  #Re-Estimate the transition probabilities (a)\n",
        "  #a_new[j, k] = (Expected number of transitions from state j to state k) / (Expected number of times in state j)\n",
        "  a = np.sum(xi, axis=0) / np.sum(gamma[:-1], axis=0, keepdims=True).T\n",
        "\n",
        "  #Re-Estimate emission prbabilities (b)\n",
        "  #Initialize b matrix with zeros\n",
        "  b = np.zeros((n,m))\n",
        "\n",
        "  #Iterate through each possible observation symbol\n",
        "  for i in range(m):\n",
        "    #Create a boolean mask that is true when the observation at time t is equal to i\n",
        "    mask = (observations == i)\n",
        "    #Sum gamma[t, j] only for those time steps t where observations[t] was i\n",
        "    #This gives the expected number of times in state j while emitting i\n",
        "    b[:,i] = np.sum(gamma[mask], axis=0)\n",
        "\n",
        "  #Sum gamma[t, j] over all time steps t (from 0 to t-1)\n",
        "  #This gives the total expected number of times in state j\n",
        "  b /= np.sum(gamma, axis=0, keepdims=True).T\n",
        "\n",
        "  return a, b, pi\n",
        "\n",
        "#Implent the Baum-Welch algorithm to learn the parameters\n",
        "def baum_welch(observations, n, m, iterations=10):\n",
        "  #Use Dirichlet distribution to initialize transition matrix, emission matrix, and the initial state distribution\n",
        "  a = np.random.dirichlet(np.ones(n), size=n)\n",
        "  b = np.random.dirichlet(np.ones(m), size=m)\n",
        "  pi = np.random.dirichlet(np.ones(n))\n",
        "\n",
        "  #Perform Expectation-Maximization\n",
        "  for i in range(iterations):\n",
        "    #Expectation step\n",
        "    gamma, xi = FindGammaXi(observations, a, b, pi)\n",
        "    #Maximization step\n",
        "    a, b, pi = updateParameters(observations, gamma, xi)\n",
        "\n",
        "  return a, b, pi\n",
        "\n",
        "#Implement the Viterbi algorithm to find the most likely sequence of hidden states\n",
        "def viterbi(a, b, pi):\n",
        "  #Number of time steps and hidden states\n",
        "  t = len(observations)\n",
        "  n = a.shape[0]\n",
        "\n",
        "  #delta[i, j] = max probability of any path that ends in state j at time i\n",
        "  delta = np.zeros((t,n))\n",
        "\n",
        "  #psi[i, j] = the state at time i-1 that led to state j at time i with the highest probability\n",
        "  psi = np.zeros((t, n), dtype=int)\n",
        "\n",
        "  #Initilization step\n",
        "  #Set initial probabilities for each state based on starting probability and emission probability of first observation\n",
        "  delta[0] = pi * b[:, observations[0]]\n",
        "\n",
        "  #Recursion step\n",
        "  for i in range(1,t): #For each time step\n",
        "    for j in range(n): #For each possible current state\n",
        "      temp = delta[i-1] * a[:,j] #Probability of each previous state transitioning to j\n",
        "      psi[i,j] = np.argmax(temp) #Most likely previous state\n",
        "      delta[i,j] = np.max(temp) * b[j, observations[i]] #max probability path to state j at time i\n",
        "\n",
        "  #Termination step\n",
        "  path = np.zeros(t, dtype=int)\n",
        "\n",
        "  #Start backtracking from the most probable final state\n",
        "  path[-1] = np.argmax([delta[-1]])\n",
        "\n",
        "  #Backtracking\n",
        "  for i in reversed(range(1,t)):\n",
        "    path[i-1] = psi[i, path[i]] #Backtrack to previous state using psi\n",
        "\n",
        "  return path\n",
        "\n",
        "\n",
        "\n",
        "observations = X.flatten()\n",
        "a, b, pi = baum_welch(observations, n=2, m=2, iterations=1000)\n",
        "hiddenStates = viterbi(a, b, pi)\n",
        "\n",
        "print(\"Estimated transition matrix:\\n\", a)\n",
        "print(\"Actual transition matrix:\\n\", transmat_true)\n",
        "print(\"Estimated emission matrix:\\n\", b)\n",
        "print(\"Actual emission matrix:\\n\", emission_probs_true)\n",
        "print(\"Estimated initial distribution:\\n\", pi)\n",
        "print(\"Actual initial distribution:\\n\", startprob_true)\n",
        "print(\"First 10 predicted hidden states:\", hiddenStates[:10])\n",
        "print(\"First 10 true hidden States:\", Z[:10])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rCGgZUWjlGK",
        "outputId": "9785c696-5640-4cff-c9f3-ad09a6b2b211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated transition matrix:\n",
            " [[0.91792298 0.08207702]\n",
            " [0.14673815 0.85326185]]\n",
            "Actual transition matrix:\n",
            " [[0.9 0.1]\n",
            " [0.1 0.9]]\n",
            "Estimated emission matrix:\n",
            " [[0.75022477 0.24977523]\n",
            " [0.27702502 0.72297498]]\n",
            "Actual emission matrix:\n",
            " [[0.8 0.2]\n",
            " [0.3 0.7]]\n",
            "Estimated initial distribution:\n",
            " [0. 1.]\n",
            "Actual initial distribution:\n",
            " [0.5 0.5]\n",
            "First 10 predicted hidden states: [1 1 1 1 0 0 0 0 0 0]\n",
            "First 10 true hidden States: [1 1 0 0 0 0 1 0 0 0]\n"
          ]
        }
      ]
    }
  ]
}